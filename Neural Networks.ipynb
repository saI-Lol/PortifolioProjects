{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53f47b6-c59d-46be-ab15-d0f63567fc66",
   "metadata": {},
   "source": [
    "# Nueral Network From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096dcf20-64b1-49bc-abdc-df6f13e9c993",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c8e9b-c372-4822-ad37-e9a8f713f8fe",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6f816d19-6400-43f9-b000-d2b1183a4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer in Network\n",
    "    \n",
    "    Parameters\n",
    "    units: number of neurons in a layer\n",
    "    activation: activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,units,activation):\n",
    "        \"\"\"constructor for Layer class\"\"\"\n",
    "        self.units=units\n",
    "        self.activation=activation\n",
    "        self.weights=None\n",
    "        self.bias=None\n",
    "        self.Z=None\n",
    "        self.dZ=None\n",
    "        self.Input=None\n",
    "        self.weights_temp=None\n",
    "        self.bias_temp=None\n",
    "        import numpy as np #import numpy package\n",
    "    \n",
    "    def initParams(self,input_shape):\n",
    "        \"\"\"initialize weights and bias\"\"\"\n",
    "        self.weights=(np.random.randn(self.units,input_shape[0]).round(2))*0.01\n",
    "        self.bias=np.zeros(self.units).reshape(-1,1)\n",
    "    \n",
    "    def getActivation(self,input_matrix):\n",
    "        \"\"\"Compute activation for a layer\"\"\"\n",
    "        Z=(np.matmul(self.weights,input_matrix))+self.bias \n",
    "        self.Z=Z #cache Z\n",
    "        self.Input=input_matrix #record input to layer\n",
    "        if (self.activation=='relu'):\n",
    "            A=self.ReLU(Z)\n",
    "        elif (self.activation=='sigmoid'):\n",
    "            A=self.Sigmoid(Z)\n",
    "        elif (self.activation=='linear'):\n",
    "            A=self.Linear(Z)\n",
    "        return A\n",
    "    \n",
    "    #Activation functions\n",
    "    def ReLU(self,Z):\n",
    "        \"\"\"Rectified Linear activation function\"\"\"\n",
    "        temp=Z.copy()\n",
    "        temp[temp<=0]=0\n",
    "        return temp\n",
    "    def Sigmoid(self,Z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    def Linear(self,Z):\n",
    "        \"\"\"Linear activation function\"\"\"\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "52f2d8f2-df48-46a7-aea0-13dab330325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    run=0 #gradient descent not yet run\n",
    "    \"\"\"\n",
    "    Network of Layers\n",
    "    \n",
    "    Parameters\n",
    "    layers: List of tuples. Each tuple represents a layer.\\\n",
    "    Each tuple contains number of units and activation for the layer.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,layers):\n",
    "        \"\"\"Constructor for Network class\"\"\"\n",
    "        self.layers=layers\n",
    "        self.Layer_objs=[]\n",
    "        \n",
    "\n",
    "    def forwardProp(self,A):\n",
    "        \"\"\"Implementation of forward propagation\"\"\"\n",
    "        if (Network.run==0): #first run of gradient descent\n",
    "            for layer in self.layers:\n",
    "                Layer_obj=Layer(layer[0],layer[1])\n",
    "                Layer_obj.initParams(A.shape)\n",
    "                A=Layer_obj.getActivation(A)\n",
    "                self.Layer_objs.append(Layer_obj)\n",
    "            Network.run=1 #gradient descent already run\n",
    "            return A\n",
    "        else: #other runs of gradient descent\n",
    "            for obj in self.Layer_objs:\n",
    "                obj.Input=A #assign value of Input variable\n",
    "                A=obj.getActivation(A)\n",
    "            return A\n",
    "                           \n",
    "    \n",
    "    def backProp(self, A, Y, alpha=0.1):\n",
    "        \"\"\"Implementation of back propagation\"\"\"\n",
    "        n=len(self.Layer_objs)\n",
    "        m=A.shape[1]\n",
    "        for i in range(-1,-(n+1),-1):\n",
    "            current_layer=self.Layer_objs[i] #layer being indexed\n",
    "            if (i==-1):\n",
    "                dZ_current_layer=A-Y\n",
    "                current_layer.dZ=dZ_current_layer\n",
    "            else: \n",
    "                layer_above=self.Layer_objs[i+1]\n",
    "                W_layer_above=layer_above.weights #obtain weight matrix of layer above\n",
    "                dZ_layer_above=layer_above.dZ #obtain dz for layer above\n",
    "                \n",
    "                #fetch differential of activation\n",
    "                if (current_layer.activation=='relu'):\n",
    "                    diff_activation=self.diffReLU(current_layer.Z)\n",
    "                elif (current_layer.activation=='sigmoid'):\n",
    "                    diff_activation=self.diffSigmoid(current_layer.Z)\n",
    "                elif (current_layer.activation=='linear'):\n",
    "                    diff_activation=self.diffLinear(current_layer.Z)\n",
    "                    \n",
    "                dZ_current_layer=(np.matmul(W_layer_above.T,dZ_layer_above)*diff_activation)\n",
    "                current_layer.dZ=dZ_current_layer\n",
    "            dW=np.matmul(dZ_current_layer,current_layer.Input.T)/m\n",
    "            db=np.sum(dZ_current_layer,axis=1,keepdims=True)/m\n",
    "            \n",
    "            current_layer.weights_temp=current_layer.weights-(alpha*dW)\n",
    "            current_layer.bias_temp=current_layer.bias-(alpha*db)\n",
    "            \n",
    "        #update weights and bias vectors\n",
    "        for obj in self.Layer_objs:\n",
    "            obj.weights=obj.weights_temp\n",
    "            obj.bias=obj.bias_temp\n",
    "            \n",
    "    #differentiate activation functions\n",
    "    def diffReLU(self,z):\n",
    "        \"\"\"differentiate relu activation function\"\"\"\n",
    "        temp=z.copy()\n",
    "        temp[temp<=0]=0\n",
    "        temp[temp>0]=1\n",
    "        return temp\n",
    "    def diffSigmoid(self,z):\n",
    "        \"\"\"differentiate sigmoid activation function\"\"\"\n",
    "        a=1/(1+np.exp(-z))\n",
    "        return a*(1-a)\n",
    "    def diffLinear(self,z):\n",
    "        \"\"\"differentiate linear activation function\"\"\"\n",
    "        m,n=z.shape\n",
    "        return np.ones((m,n))\n",
    "    \n",
    "    \n",
    "    def gradientDescent(self, n_iter, X, Y, alpha):\n",
    "        \"\"\"Run gradient descent algorithm\"\"\"\n",
    "        for i in range(n_iter):\n",
    "            A=self.forwardProp(X)\n",
    "            print(f'Iteration: {i+1}\\tCost: {self.logLoss(A,Y)}')\n",
    "            self.backProp(A,Y,alpha)\n",
    "        Network.run=0\n",
    "        return\n",
    "            \n",
    "            \n",
    "    def logLoss(self,A,Y):\n",
    "        \"\"\"Calculate cost\"\"\"\n",
    "        m=A.shape[1]\n",
    "        loss=-((Y*np.log(A))+((1-Y)*np.log(1-A)))\n",
    "        cost=(np.sum(loss))/m\n",
    "        return cost\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
